{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, cv2\nimport numpy as np\nimport pandas as pd\nimport random, tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport albumentations as album","metadata":{"_uuid":"7ef94575-2c5a-4868-b1d2-f6836c7b9a6f","_cell_guid":"cb7f05b7-1fe6-45fc-a99d-f4f8fc552658","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:43:53.876429Z","iopub.execute_input":"2021-12-05T06:43:53.876793Z","iopub.status.idle":"2021-12-05T06:43:56.546684Z","shell.execute_reply.started":"2021-12-05T06:43:53.876747Z","shell.execute_reply":"2021-12-05T06:43:56.545853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\nimport segmentation_models_pytorch as smp","metadata":{"_uuid":"6e549d29-8a22-4fca-ae6a-bb12bdb0a4f3","_cell_guid":"e7dfc7b0-4837-4dd2-984b-cde67724f7de","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:43:56.548597Z","iopub.execute_input":"2021-12-05T06:43:56.548963Z","iopub.status.idle":"2021-12-05T06:44:15.205616Z","shell.execute_reply.started":"2021-12-05T06:43:56.548925Z","shell.execute_reply":"2021-12-05T06:44:15.204565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining train / val / test directories üìÅ","metadata":{"_uuid":"c160248e-9bf2-4ead-b7bf-33b422c65b5d","_cell_guid":"86201227-e76a-453e-8922-c8e7d5e06d8b","trusted":true}},{"cell_type":"code","source":"DATA_DIR = '//path to /massachusetts-roads-dataset/tiff/ present in Github dataset file'\n\nx_train_dir = os.path.join(DATA_DIR, 'train')\ny_train_dir = os.path.join(DATA_DIR, 'train_labels')\n\nx_valid_dir = os.path.join(DATA_DIR, 'val')\ny_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n\nx_test_dir = os.path.join(DATA_DIR, 'test')\ny_test_dir = os.path.join(DATA_DIR, 'test_labels')","metadata":{"_uuid":"37339022-3d22-4ef0-9f4d-b9b35edaf93c","_cell_guid":"eee00fb2-fb1c-486f-a7e4-eb421290e964","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:15.207395Z","iopub.execute_input":"2021-12-05T06:44:15.207797Z","iopub.status.idle":"2021-12-05T06:44:15.216748Z","shell.execute_reply.started":"2021-12-05T06:44:15.207752Z","shell.execute_reply":"2021-12-05T06:44:15.215306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_dict = pd.read_csv(\"// path to /massachusetts-roads-dataset/label_class_dict.csv/ present in Github Dataset\")\n# Get class names\nclass_names = class_dict['name'].tolist()\n# Get class RGB values\nclass_rgb_values = class_dict[['r','g','b']].values.tolist()\n\nprint('All dataset classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"_uuid":"a0eac2cb-94bd-4e9e-a179-6d77a41d6af2","_cell_guid":"2686fb6f-af5c-4fc0-ab9c-c22aee69d477","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:15.218305Z","iopub.execute_input":"2021-12-05T06:44:15.219Z","iopub.status.idle":"2021-12-05T06:44:15.250099Z","shell.execute_reply.started":"2021-12-05T06:44:15.218941Z","shell.execute_reply":"2021-12-05T06:44:15.248912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shortlist specific classes to segment","metadata":{"_uuid":"1fca9e62-1729-400d-9c29-32fd71fb120a","_cell_guid":"672fd987-dee8-4208-87d1-202d2747dce6","trusted":true}},{"cell_type":"code","source":"# Useful to shortlist specific classes in datasets with large number of classes\nselect_classes = ['background', 'road']\n\n# Get RGB values of required classes\nselect_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\nselect_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n\nprint('Selected classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"_uuid":"12e93edb-80b8-4140-843c-65b15b6b9bb1","_cell_guid":"81ed738a-4d24-4807-8d93-0c73e933fd36","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:15.253715Z","iopub.execute_input":"2021-12-05T06:44:15.254002Z","iopub.status.idle":"2021-12-05T06:44:15.263141Z","shell.execute_reply.started":"2021-12-05T06:44:15.253975Z","shell.execute_reply":"2021-12-05T06:44:15.261837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper functions for viz. & one-hot encoding/decoding","metadata":{"_uuid":"05d4ce59-9f7d-4025-bbda-926d03f4124d","_cell_guid":"15e8392e-f837-4c46-8325-78e9ef7d0d10","trusted":true}},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"\n    Plot images in one row\n    \"\"\"\n    n_images = len(images)\n    plt.figure(figsize=(20,8))\n    for idx, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n_images, idx + 1)\n        plt.xticks([]); \n        plt.yticks([])\n        # get title from the parameter names\n        plt.title(name.replace('_',' ').title(), fontsize=20)\n        plt.imshow(image)\n    plt.show()\n\n# Perform one hot encoding on label\ndef one_hot_encode(label, label_values):\n    \"\"\"\n    Convert a segmentation image label array to one-hot format\n    by replacing each pixel value with a vector of length num_classes\n    # Arguments\n        label: The 2D array segmentation image label\n        label_values\n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of num_classes\n    \"\"\"\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis = -1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1)\n\n    return semantic_map\n    \n# Perform reverse one-hot-encoding on labels / preds\ndef reverse_one_hot(image):\n    \"\"\"\n    Transform a 2D array in one-hot format (depth is num_classes),\n    to a 2D array with only 1 channel, where each pixel value is\n    the classified class key.\n    # Arguments\n        image: The one-hot format image \n        \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of 1, where each pixel value is the classified \n        class key.\n    \"\"\"\n    x = np.argmax(image, axis = -1)\n    return x\n\n# Perform colour coding on the reverse-one-hot outputs\ndef colour_code_segmentation(image, label_values):\n    \"\"\"\n    Given a 1-channel array of class keys, colour code the segmentation results.\n    # Arguments\n        image: single channel array where each value represents the class key.\n        label_values\n\n    # Returns\n        Colour coded image for segmentation visualization\n    \"\"\"\n    colour_codes = np.array(label_values)\n    x = colour_codes[image.astype(int)]\n\n    return x","metadata":{"_uuid":"faf5cc01-36a7-4089-8091-f799a033f25d","_cell_guid":"dac4d953-1525-448b-aec0-b98d25afd62f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:15.266631Z","iopub.execute_input":"2021-12-05T06:44:15.267077Z","iopub.status.idle":"2021-12-05T06:44:15.280867Z","shell.execute_reply.started":"2021-12-05T06:44:15.267047Z","shell.execute_reply":"2021-12-05T06:44:15.279995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoadsDataset(torch.utils.data.Dataset):\n\n    \"\"\"Massachusetts Roads Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    def __init__(\n            self, \n            images_dir, \n            masks_dir, \n            class_rgb_values=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        \n        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n\n        self.class_rgb_values = class_rgb_values\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read images and masks\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n        \n        # one-hot-encode the mask\n        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        # return length of \n        return len(self.image_paths)","metadata":{"_uuid":"cd0007fd-4671-4abd-9f06-be4628d38c1f","_cell_guid":"0ddc6603-0d9f-45fb-8ec2-d7720032cd75","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:15.282521Z","iopub.execute_input":"2021-12-05T06:44:15.283196Z","iopub.status.idle":"2021-12-05T06:44:15.299126Z","shell.execute_reply.started":"2021-12-05T06:44:15.283157Z","shell.execute_reply":"2021-12-05T06:44:15.298293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize Sample Image and Mask üìà","metadata":{"_uuid":"61920363-6abb-4ddb-92a5-4596c7747b63","_cell_guid":"edfc3b1f-ad74-418c-9581-4a74adb90a4d","trusted":true}},{"cell_type":"code","source":"dataset = RoadsDataset(x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values)\nrandom_idx = random.randint(0, len(dataset)-1)\nimage, mask = dataset[2]\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)","metadata":{"_uuid":"eb67d42d-5fae-41ce-b193-76e149d4a6f6","_cell_guid":"66d13ed5-c118-48e0-ac67-8b44d49d44b3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:15.300787Z","iopub.execute_input":"2021-12-05T06:44:15.301346Z","iopub.status.idle":"2021-12-05T06:44:17.00247Z","shell.execute_reply.started":"2021-12-05T06:44:15.301271Z","shell.execute_reply":"2021-12-05T06:44:17.00159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining Augmentations üôÉ","metadata":{"_uuid":"057142c6-811f-4604-b660-8672fa28a70a","_cell_guid":"a42eb9ff-ccee-42f0-8dac-2c755b7e1836","trusted":true}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [    \n        album.RandomCrop(height=256, width=256, always_apply=True),\n        album.OneOf(\n            [\n                album.HorizontalFlip(p=1),\n                album.VerticalFlip(p=1),\n                album.RandomRotate90(p=1),\n            ],\n            p=0.75,\n        ),\n    ]\n    return album.Compose(train_transform)\n\n\ndef get_validation_augmentation():   \n    # Add sufficient padding to ensure image is divisible by 32\n    test_transform = [\n        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n    ]\n    return album.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn=None):\n    \"\"\"Construct preprocessing transform    \n    Args:\n        preprocessing_fn (callable): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \"\"\"   \n    _transform = []\n    if preprocessing_fn:\n        _transform.append(album.Lambda(image=preprocessing_fn))\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n        \n    return album.Compose(_transform)","metadata":{"_uuid":"2a29f24b-498e-44bf-a54f-88db8f2dae8f","_cell_guid":"28bd111c-35cc-40a2-ac36-36627294ebb1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:17.003553Z","iopub.execute_input":"2021-12-05T06:44:17.003944Z","iopub.status.idle":"2021-12-05T06:44:17.018295Z","shell.execute_reply.started":"2021-12-05T06:44:17.003908Z","shell.execute_reply":"2021-12-05T06:44:17.017349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize Augmented Images & Masks","metadata":{"_uuid":"ec7e2a4a-044e-4d14-a73a-7c4ef4794084","_cell_guid":"569976b5-877a-436b-a950-fa2d111644a5","trusted":true}},{"cell_type":"code","source":"augmented_dataset = RoadsDataset(\n    x_train_dir, y_train_dir, \n    augmentation=get_training_augmentation(),\n    class_rgb_values=select_class_rgb_values,\n)\n\nrandom_idx = random.randint(0, len(augmented_dataset)-1)\n\n# Different augmentations on a random image/mask pair (256*256 crop)\nfor i in range(3):\n    image, mask = augmented_dataset[random_idx]\n    visualize(\n        original_image = image,\n        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n        one_hot_encoded_mask = reverse_one_hot(mask)\n    )","metadata":{"_uuid":"8a33a515-7f4c-4518-8dbe-22bbf31a0946","_cell_guid":"cf3a03d9-42dd-4fa9-a7ce-8c3377960881","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:17.019794Z","iopub.execute_input":"2021-12-05T06:44:17.02052Z","iopub.status.idle":"2021-12-05T06:44:18.377499Z","shell.execute_reply.started":"2021-12-05T06:44:17.020475Z","shell.execute_reply":"2021-12-05T06:44:18.376633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training UNet","metadata":{"_uuid":"103af152-7ba4-4f7f-9237-a94b7f70c8f9","_cell_guid":"974b6f79-86ba-4f5a-9641-196106e02b5c","trusted":true}},{"cell_type":"markdown","source":"<h3><center>UNet Model Architecture</center></h3>\n<img src=\"https://miro.medium.com/max/2824/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"750\" height=\"750\"/>\n<h4><center><a href=\"https://arxiv.org/abs/1505.04597\">Image Courtesy: UNet [Ronneberger et al.]</a></center></h4>","metadata":{"_uuid":"2480015c-ed4c-4ab3-a0d5-e6a7229866cf","_cell_guid":"4f8ff0d2-acee-4f4f-a01c-d296d22283d5","trusted":true}},{"cell_type":"markdown","source":"### Model Definition","metadata":{"_uuid":"e68ee50c-74b2-44e0-b2b5-05c0db8e2b06","_cell_guid":"86c40ad4-788e-4ef8-9ffe-c2283c5a7ec9","trusted":true}},{"cell_type":"code","source":"ENCODER = 'resnet50'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = select_classes\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n\n# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"_uuid":"6546803d-c7c5-45f2-8eab-1dcf17254c07","_cell_guid":"8b7fd19c-ba9b-4e02-9671-2884353ab322","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:18.378927Z","iopub.execute_input":"2021-12-05T06:44:18.379499Z","iopub.status.idle":"2021-12-05T06:44:20.279627Z","shell.execute_reply.started":"2021-12-05T06:44:18.379458Z","shell.execute_reply":"2021-12-05T06:44:20.278826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get Train / Val DataLoaders","metadata":{"_uuid":"5e598966-12a1-4fef-9aa1-4ae8f95822a5","_cell_guid":"24b956f3-1c52-4ab3-9658-d1f39b5c7bf8","trusted":true}},{"cell_type":"code","source":"# Get train and val dataset instances\ntrain_dataset = RoadsDataset(\n    x_train_dir, y_train_dir, \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\nvalid_dataset = RoadsDataset(\n    x_valid_dir, y_valid_dir, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\n# Get train and val data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)","metadata":{"_uuid":"b4cf8499-b9e0-4673-a8cf-1776afcca43d","_cell_guid":"73492dc7-e5b6-45ee-a60f-644a46b2dd5d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:20.283163Z","iopub.execute_input":"2021-12-05T06:44:20.283459Z","iopub.status.idle":"2021-12-05T06:44:20.309655Z","shell.execute_reply.started":"2021-12-05T06:44:20.283432Z","shell.execute_reply":"2021-12-05T06:44:20.308971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Set Hyperparams","metadata":{"_uuid":"8e57bdd9-0e14-4e43-8b20-4126c4f39f8b","_cell_guid":"214f956f-e868-4068-b4f1-b4cff3cedb7a","trusted":true}},{"cell_type":"code","source":"# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\nTRAINING = True\n\n# Set num of epochs\nEPOCHS = 40\n\n# Set device: `cuda`\nDEVICE = torch.device(\"cuda:0\")\n\n# define loss function\nloss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\n# define optimizer\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.00008),\n])\n\n# define learning rate scheduler (not used in this NB)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n)\n\n# load best saved model checkpoint from previous commit (if present)\nif os.path.exists('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth'):\n    model = torch.load('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth', map_location=DEVICE)","metadata":{"_uuid":"d181f427-f8b5-4f86-85d9-c2712b6240e4","_cell_guid":"cab9b6e4-9c7d-4e2b-b849-f99df3cc0633","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:20.310923Z","iopub.execute_input":"2021-12-05T06:44:20.311283Z","iopub.status.idle":"2021-12-05T06:44:26.576455Z","shell.execute_reply.started":"2021-12-05T06:44:20.311245Z","shell.execute_reply":"2021-12-05T06:44:26.575638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","metadata":{"_uuid":"65ecd0c7-454d-4f18-8b89-450225802c21","_cell_guid":"b47f64aa-714a-4d1d-a86f-2470e3b06b46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:26.579997Z","iopub.execute_input":"2021-12-05T06:44:26.580273Z","iopub.status.idle":"2021-12-05T06:44:26.596822Z","shell.execute_reply.started":"2021-12-05T06:44:26.580246Z","shell.execute_reply":"2021-12-05T06:44:26.595947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training UNet","metadata":{"_uuid":"be92b101-4523-4b5f-80ab-8af9b5e72288","_cell_guid":"c426eb36-59a2-4df8-94bd-8d10ce96c0d2","trusted":true}},{"cell_type":"code","source":"%%time\n\nif TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(train_loader)\n        valid_logs = valid_epoch.run(valid_loader)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, './best_model.pth')\n            print('Model saved!')","metadata":{"_uuid":"65911e72-3d33-44a4-9dd0-87e8707b3095","_cell_guid":"d4ab5f45-5122-4599-a33d-347b713a1d95","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:44:26.59809Z","iopub.execute_input":"2021-12-05T06:44:26.598556Z","iopub.status.idle":"2021-12-05T06:57:07.448607Z","shell.execute_reply.started":"2021-12-05T06:44:26.598511Z","shell.execute_reply":"2021-12-05T06:57:07.44781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction on Test Data","metadata":{"_uuid":"346758db-6847-467c-a312-f1941c6f6c0c","_cell_guid":"2db9ddcc-3a10-42c6-bf9f-1ea27079014c","trusted":true}},{"cell_type":"code","source":"# load best saved model checkpoint from the current run\nif os.path.exists('./best_model.pth'):\n    best_model = torch.load('./best_model.pth', map_location=DEVICE)\n    print('Loaded UNet model from this run.')\n\n# load best saved model checkpoint from previous commit (if present)\nelif os.path.exists('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth'):\n    best_model = torch.load('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth', map_location=DEVICE)\n    print('Loaded UNet model from a previous commit.')","metadata":{"_uuid":"31319b5b-3408-4892-a542-d2d3432a524c","_cell_guid":"eb77eb87-9b85-4581-9b21-c2b15248ce0f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:57:07.450151Z","iopub.execute_input":"2021-12-05T06:57:07.450428Z","iopub.status.idle":"2021-12-05T06:57:07.588241Z","shell.execute_reply.started":"2021-12-05T06:57:07.450397Z","shell.execute_reply":"2021-12-05T06:57:07.587182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create test dataloader to be used with UNet model (with preprocessing operation: to_tensor(...))\ntest_dataset = RoadsDataset(\n    x_test_dir, \n    y_test_dir, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n)\n\ntest_dataloader = DataLoader(test_dataset)\n\n# test dataset for visualization (without preprocessing transformations)\ntest_dataset_vis = RoadsDataset(\n    x_test_dir, y_test_dir, \n    augmentation=get_validation_augmentation(),\n    class_rgb_values=select_class_rgb_values,\n)\n\n# get a random test image/mask index\nrandom_idx = random.randint(0, len(test_dataset_vis)-1)\nimage, mask = test_dataset_vis[random_idx]\n\nvisualize(\n    original_image = image,\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)\n)\n\n# Notice the images / masks are 1536*1536 because of 18px padding on all sides. \n# This is to ensure the input image dimensions to UNet model are a multiple of 2 (to account for pooling & transpose conv. operations).","metadata":{"_uuid":"fdf55b0a-aab3-4e36-89c2-1e86f962dfd3","_cell_guid":"604b4834-6730-4571-8070-fac9756dbd8f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:57:07.58975Z","iopub.execute_input":"2021-12-05T06:57:07.59028Z","iopub.status.idle":"2021-12-05T06:57:08.85929Z","shell.execute_reply.started":"2021-12-05T06:57:07.590238Z","shell.execute_reply":"2021-12-05T06:57:08.855405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Center crop padded image / mask to original image dims\ndef crop_image(image, target_image_dims=[1500,1500,3]):\n   \n    target_size = target_image_dims[0]\n    image_size = len(image)\n    padding = (image_size - target_size) // 2\n\n    if padding<0:\n        return image\n\n    return image[\n        padding:image_size - padding,\n        padding:image_size - padding,\n        :,\n    ]","metadata":{"_uuid":"6fd2f538-333b-4d54-8ebb-e5b6344cc9c0","_cell_guid":"3bc1c4ce-196a-4d45-929b-5adcafc510f4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:57:08.860847Z","iopub.execute_input":"2021-12-05T06:57:08.861499Z","iopub.status.idle":"2021-12-05T06:57:08.869337Z","shell.execute_reply.started":"2021-12-05T06:57:08.861455Z","shell.execute_reply":"2021-12-05T06:57:08.868266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_preds_folder = 'sample_predictions/'\nif not os.path.exists(sample_preds_folder):\n    os.makedirs(sample_preds_folder)","metadata":{"_uuid":"18beabe9-5500-4e1f-ad97-050a86e2f234","_cell_guid":"4ed67dd3-8e27-48f2-9fba-c0760d92fbd8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:57:08.871305Z","iopub.execute_input":"2021-12-05T06:57:08.871709Z","iopub.status.idle":"2021-12-05T06:57:08.883088Z","shell.execute_reply.started":"2021-12-05T06:57:08.87167Z","shell.execute_reply":"2021-12-05T06:57:08.882013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in range(len(test_dataset)):\n\n    image, gt_mask = test_dataset[idx]\n    image_vis = crop_image(test_dataset_vis[idx][0].astype('uint8'))\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    # Predict test image\n    pred_mask = best_model(x_tensor)\n    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n    # Convert pred_mask from `CHW` format to `HWC` format\n    pred_mask = np.transpose(pred_mask,(1,2,0))\n    # Get prediction channel corresponding to road\n    pred_road_heatmap = pred_mask[:,:,select_classes.index('road')]\n    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n    # Convert gt_mask from `CHW` format to `HWC` format\n    gt_mask = np.transpose(gt_mask,(1,2,0))\n    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n    \n    visualize(\n        original_image = image_vis,\n        ground_truth_mask = gt_mask,\n        predicted_mask = pred_mask,\n        predicted_road_heatmap = pred_road_heatmap\n    )","metadata":{"_uuid":"93277bf2-1282-4633-8710-ec379e7e4d86","_cell_guid":"0e370460-7522-4354-9b85-5c6f3f2f9e79","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:57:08.884971Z","iopub.execute_input":"2021-12-05T06:57:08.885621Z","iopub.status.idle":"2021-12-05T06:58:51.726495Z","shell.execute_reply.started":"2021-12-05T06:57:08.885576Z","shell.execute_reply":"2021-12-05T06:58:51.725677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation on Test Dataset","metadata":{"_uuid":"7336fb6f-a7f3-4d93-97ec-4af448fd7edb","_cell_guid":"37f18fdb-3fb6-485a-beb6-1815076405b4","trusted":true}},{"cell_type":"code","source":"test_epoch = smp.utils.train.ValidEpoch(\n    model,\n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_logs = test_epoch.run(test_dataloader)\nprint(\"Evaluation on Test Data: \")\nprint(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\nprint(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")","metadata":{"_uuid":"c848b772-aa33-4301-8a72-9e36ee4e1b69","_cell_guid":"cc3cbe1f-0d4f-452d-81ec-a4a20d4dc30b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:58:51.727861Z","iopub.execute_input":"2021-12-05T06:58:51.728427Z","iopub.status.idle":"2021-12-05T06:59:11.784267Z","shell.execute_reply.started":"2021-12-05T06:58:51.728388Z","shell.execute_reply":"2021-12-05T06:59:11.783379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Dice Loss & IoU Metric for Train vs. Val","metadata":{"_uuid":"54fc9f76-71b7-4c90-b3ee-e14ef47f855f","_cell_guid":"af6c572f-75fc-40bf-9b68-9306e52b99fe","trusted":true}},{"cell_type":"code","source":"train_logs_df = pd.DataFrame(train_logs_list)\nvalid_logs_df = pd.DataFrame(valid_logs_list)\ntrain_logs_df.T","metadata":{"_uuid":"a041dace-dc87-42e6-9e8c-a06a056d35a3","_cell_guid":"0b77a75c-1400-434a-b379-5484a30efbc1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:59:11.785873Z","iopub.execute_input":"2021-12-05T06:59:11.786269Z","iopub.status.idle":"2021-12-05T06:59:11.820822Z","shell.execute_reply.started":"2021-12-05T06:59:11.786227Z","shell.execute_reply":"2021-12-05T06:59:11.81985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=21)\nplt.ylabel('IoU Score', fontsize=21)\nplt.title('IoU Score Plot', fontsize=21)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('iou_score_plot.png')\nplt.show()","metadata":{"_uuid":"17e6ef4a-428b-4aed-b668-040463bfac40","_cell_guid":"3ea87c54-a1fd-498c-a060-587ceb7b96df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:59:11.822774Z","iopub.execute_input":"2021-12-05T06:59:11.823215Z","iopub.status.idle":"2021-12-05T06:59:12.168995Z","shell.execute_reply.started":"2021-12-05T06:59:11.823176Z","shell.execute_reply":"2021-12-05T06:59:12.16797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=21)\nplt.ylabel('Dice Loss', fontsize=21)\nplt.title('Dice Loss Plot', fontsize=21)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('dice_loss_plot.png')\nplt.show()","metadata":{"_uuid":"2d55fa1e-c2bf-487c-8ee8-34224ff08262","_cell_guid":"ebc2700b-3543-45b3-a75b-c155234020f1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-05T06:59:12.170441Z","iopub.execute_input":"2021-12-05T06:59:12.171073Z","iopub.status.idle":"2021-12-05T06:59:12.493889Z","shell.execute_reply.started":"2021-12-05T06:59:12.17101Z","shell.execute_reply":"2021-12-05T06:59:12.49286Z"},"trusted":true},"execution_count":null,"outputs":[]}]}